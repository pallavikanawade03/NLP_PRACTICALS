{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ASS NO-7 Write a better auto-complete algorithm using an N-gram model (similar models are used for translation, determining the author of a text, and speech recognition)**"
      ],
      "metadata": {
        "id": "Ip307OpfENrj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "062Tx3RaEHJB"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Imports complete.')\n",
        "# Tokenize text: lowercase, remove non-letter characters, and split into words\n",
        "def tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18K-sjPzEbq-",
        "outputId": "ee378b42-3a12-4c1b-9b6d-c929cb90c242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imports complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "sample_text = \"Hello, world! This is a test.\"\n",
        "tokens = tokenize(sample_text)\n",
        "print('Tokens:', tokens)\n",
        "# Build an n-gram model (n must be >= 2)\n",
        "def build_ngram_model(tokens, n):\n",
        "    model = defaultdict(Counter)\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        context = tuple(tokens[i:i+n-1])\n",
        "        next_word = tokens[i+n-1]\n",
        "        model[context][next_word] += 1\n",
        "    return model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8paSUOnLEfXk",
        "outputId": "9b8afb58-bc9f-4ee7-d176-cc28dcac5117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['hello', 'world', 'this', 'is', 'a', 'test']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a trigram (n=3) model for demonstration\n",
        "corpus = \"Once upon a time in a land far away there was a mysterious forest. The forest was filled with ancient trees and hidden secrets. Many travelers ventured into the forest seeking adventure and wisdom. Some never returned and their stories became legends.\"\n",
        "tokens = tokenize(corpus)\n",
        "n = 3\n",
        "ngram_model = build_ngram_model(tokens, n)\n",
        "\n",
        "print('Trigram model built. Sample context and counts:')\n",
        "sample_context = tuple(tokens[0:n-1])\n",
        "print(sample_context, ngram_model[sample_context])\n",
        "# Predict next words given a context using add-one smoothing\n",
        "def predict_next_words(model, context, n, top_k=5):\n",
        "    if len(context) != n - 1:\n",
        "        raise ValueError(f\"Context must have {n-1} words\")\n",
        "    context = tuple(context)\n",
        "    predictions = model.get(context, {})\n",
        "\n",
        "    # Build vocabulary from all counts in the model\n",
        "    vocab = set()\n",
        "    for counts in model.values():\n",
        "        vocab.update(counts.keys())\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Total count for this context\n",
        "    total = sum(predictions.values())\n",
        "\n",
        "    # Compute probability with Laplace smoothing\n",
        "    scores = {}\n",
        "    for word in vocab:\n",
        "        count = predictions.get(word, 0)\n",
        "        scores[word] = (count + 1) / (total + vocab_size)\n",
        "\n",
        "    # Return the top_k words sorted by probability\n",
        "    sorted_preds = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_preds[:top_k]\n",
        "\n",
        "# Example prediction using context from the corpus\n",
        "context = [\"the\", \"forest\"]\n",
        "predictions = predict_next_words(ngram_model, context, n, top_k=5)\n",
        "print('Auto-complete predictions for context', context)\n",
        "for word, prob in predictions:\n",
        "    print(f\"{word}: {prob:.3f}\")\n",
        "# Function to simulate auto-completion\n",
        "def auto_complete(model, context, n, max_words=5):\n",
        "    completed = context.copy()\n",
        "    for _ in range(max_words):\n",
        "        predictions = predict_next_words(model, completed[-(n-1):], n, top_k=1)\n",
        "        if not predictions:\n",
        "            break\n",
        "        next_word = predictions[0][0]\n",
        "        completed.append(next_word)\n",
        "    return ' '.join(completed)\n",
        "\n",
        "# Demo auto-completion starting from a given context\n",
        "start_context = [\"many\", \"travelers\"]\n",
        "completed_text = auto_complete(ngram_model, start_context, n, max_words=10)\n",
        "print(\"Auto-completed text:\", completed_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjg8agJmEi6m",
        "outputId": "ccc255c1-c8c4-4fdb-db1a-914dcec8f1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram model built. Sample context and counts:\n",
            "('once', 'upon') Counter({'a': 1})\n",
            "Auto-complete predictions for context ['the', 'forest']\n",
            "was: 0.059\n",
            "seeking: 0.059\n",
            "secrets: 0.029\n",
            "their: 0.029\n",
            "away: 0.029\n",
            "Auto-completed text: many travelers ventured into the forest was filled with ancient trees and\n"
          ]
        }
      ]
    }
  ]
}