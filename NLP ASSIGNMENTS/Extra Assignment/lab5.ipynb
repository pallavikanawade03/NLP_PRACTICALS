{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab') # This line was added to download the missing data\n",
        "\n",
        "# Sample training text (can be replaced with a file or corpus)\n",
        "text = \"\"\"\n",
        "Natural language processing is a subfield of linguistics computer science and artificial intelligence.\n",
        "It is concerned with the interactions between computers and human language.\n",
        "In particular, how to program computers to process and analyze large amounts of natural language data.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Function to build n-gram model with Laplace smoothing\n",
        "def build_ngram_model(tokens, n):\n",
        "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "    padded_tokens = ['<s>'] * (n - 1) + tokens + ['</s>']\n",
        "\n",
        "    for i in range(len(padded_tokens) - n + 1):\n",
        "        context = tuple(padded_tokens[i:i + n - 1])\n",
        "        word = padded_tokens[i + n - 1]\n",
        "        model[context][word] += 1\n",
        "\n",
        "    # Apply Laplace smoothing\n",
        "    for context in model:\n",
        "        total_count = sum(model[context].values()) + len(model[context])\n",
        "        for word in model[context]:\n",
        "            model[context][word] = (model[context][word] + 1) / total_count\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to compute probability of a sequence\n",
        "def compute_sequence_probability(model, sequence, n):\n",
        "    padded_seq = ['<s>'] * (n - 1) + word_tokenize(sequence.lower()) + ['</s>']\n",
        "    prob = 1.0\n",
        "\n",
        "    for i in range(len(padded_seq) - n + 1):\n",
        "        context = tuple(padded_seq[i:i + n - 1])\n",
        "        word = padded_seq[i + n - 1]\n",
        "        word_prob = model[context].get(word, 1 / (sum(model[context].values()) + 1))\n",
        "        prob *= word_prob\n",
        "\n",
        "    return prob\n",
        "\n",
        "# Build unigram, bigram, trigram models\n",
        "unigram_model = build_ngram_model(tokens, 1)\n",
        "bigram_model = build_ngram_model(tokens, 2)\n",
        "trigram_model = build_ngram_model(tokens, 3)\n",
        "\n",
        "# Test sentence\n",
        "test_sentence = \"natural language processing\"\n",
        "\n",
        "# Compute probabilities\n",
        "print(f\"\\nUnigram Prob: {compute_sequence_probability(unigram_model, test_sentence, 1)}\")\n",
        "print(f\"Bigram Prob: {compute_sequence_probability(bigram_model, test_sentence, 2)}\")\n",
        "print(f\"Trigram Prob: {compute_sequence_probability(trigram_model, test_sentence, 3)}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03hOOSrsBv0a",
        "outputId": "6ba83e56-068c-4e95-ba64-22a2f56021b1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unigram Prob: 1.2323466028222122e-06\n",
            "Bigram Prob: 0.16666666666666666\n",
            "Trigram Prob: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LQcidSYYBwPK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}