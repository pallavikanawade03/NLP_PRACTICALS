{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ASS NO-3 Perform text cleaning, perform lemmatization (any method), remove stop words (any method), label encoding. Create representations using TF-IDF. Save outputs**"
      ],
      "metadata": {
        "id": "U-fG4OFAAxGk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install-dependencies",
        "outputId": "a6abb912-9358-4afc-b501-0a1950a45a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk scikit-learn pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "import-libraries",
        "outputId": "2fb592aa-3804-4b46-d13d-729fa9510b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported and NLTK resources downloaded.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Added to resolve the LookupError\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print('Libraries imported and NLTK resources downloaded.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "create-and-clean-data",
        "outputId": "d6df6128-10ba-475e-bdea-da5e3c3e9721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data:\n",
            "                                        text label\n",
            "0  This is the first document. It's amazing!     A\n",
            "1      This document is the second document.     B\n",
            "2    And this is the third one? Yes, indeed!     A\n",
            "3    Is this the first document? Absolutely.     B\n",
            "\n",
            "Cleaned Text:\n",
            "                                        text  \\\n",
            "0  This is the first document. It's amazing!   \n",
            "1      This document is the second document.   \n",
            "2    And this is the third one? Yes, indeed!   \n",
            "3    Is this the first document? Absolutely.   \n",
            "\n",
            "                               clean_text  \n",
            "0  this is the first document its amazing  \n",
            "1    this document is the second document  \n",
            "2    and this is the third one yes indeed  \n",
            "3   is this the first document absolutely  \n"
          ]
        }
      ],
      "source": [
        "# Create a sample dataset\n",
        "data = {\n",
        "    'text': [\n",
        "        \"This is the first document. It's amazing!\",\n",
        "        \"This document is the second document.\",\n",
        "        \"And this is the third one? Yes, indeed!\",\n",
        "        \"Is this the first document? Absolutely.\"\n",
        "    ],\n",
        "    'label': ['A', 'B', 'A', 'B']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print('Original Data:')\n",
        "print(df)\n",
        "\n",
        "# Function for text cleaning: lowercasing, removing digits & punctuation, and trimming spaces\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "print('\\nCleaned Text:')\n",
        "print(df[['text', 'clean_text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lemmatize-remove-stopwords",
        "outputId": "1e6dbb2e-8052-4fc2-9e50-2a70899cad0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatized Text (stop words removed):\n",
            "                               clean_text                 lemmatized\n",
            "0  this is the first document its amazing     first document amazing\n",
            "1    this document is the second document   document second document\n",
            "2    and this is the third one yes indeed       third one yes indeed\n",
            "3   is this the first document absolutely  first document absolutely\n"
          ]
        }
      ],
      "source": [
        "# Initialize the lemmatizer and define stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function for lemmatization with stop word removal\n",
        "def lemmatize_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['lemmatized'] = df['clean_text'].apply(lemmatize_text)\n",
        "print('\\nLemmatized Text (stop words removed):')\n",
        "print(df[['clean_text', 'lemmatized']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "label-encode-tfidf-save",
        "outputId": "2d5b1e36-67dd-4680-cc4e-26b4ea365b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Label Encoded Data:\n",
            "  label  label_encoded\n",
            "0     A              0\n",
            "1     B              1\n",
            "2     A              0\n",
            "3     B              1\n",
            "\n",
            "TF-IDF Representation Shape:\n",
            "(4, 9)\n",
            "\n",
            "Outputs saved: processed_text.csv and tfidf_matrix.pkl\n"
          ]
        }
      ],
      "source": [
        "# Label Encoding for the labels\n",
        "le = LabelEncoder()\n",
        "df['label_encoded'] = le.fit_transform(df['label'])\n",
        "\n",
        "print('\\nLabel Encoded Data:')\n",
        "print(df[['label', 'label_encoded']])\n",
        "\n",
        "# Create TF-IDF representations for the lemmatized text\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(df['lemmatized'])\n",
        "\n",
        "print('\\nTF-IDF Representation Shape:')\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "# Save outputs to disk\n",
        "df.to_csv('processed_text.csv', index=False)\n",
        "with open('tfidf_matrix.pkl', 'wb') as f:\n",
        "    pickle.dump(tfidf_matrix, f)\n",
        "\n",
        "print('\\nOutputs saved: processed_text.csv and tfidf_matrix.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-95sIyGfTXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}