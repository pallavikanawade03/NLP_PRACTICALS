{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ASS NO-2 Perform bag-of-words approach (count occurrence, normalized count occurrence), TF-IDF on data. Create embeddings using Word2Vec**"
      ],
      "metadata": {
        "id": "mg83qPjBADPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksbpSBTGZmGR",
        "outputId": "4f2e73ef-2309-4cc5-82ed-c39fef57d553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aydhleADVWXC",
        "outputId": "8162bd73-1968-4659-e481-f37da844d43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bag-of-Words (BoW):\n",
            "    amazing  crucial  deep  embeddings  improve  is  language  learning  \\\n",
            "0        1        0     0           0        0   1         0         1   \n",
            "1        0        0     1           0        0   1         0         2   \n",
            "2        0        0     0           0        0   0         1         1   \n",
            "3        0        0     0           1        1   0         0         0   \n",
            "4        0        1     0           0        0   1         0         0   \n",
            "\n",
            "   machine  models  natural  nlp  of  part  processing  text  uses  word  \n",
            "0        1       0        0    0   0     0           0     0     0     0  \n",
            "1        1       0        0    0   1     1           0     0     0     0  \n",
            "2        1       0        1    0   0     0           1     0     1     0  \n",
            "3        0       1        0    1   0     0           0     0     0     1  \n",
            "4        0       0        0    1   1     1           1     1     0     0  \n",
            "\n",
            "Normalized Term Frequency (TF):\n",
            "    amazing   crucial      deep  embeddings  improve        is  language  \\\n",
            "0     0.25  0.000000  0.000000         0.0      0.0  0.250000  0.000000   \n",
            "1     0.00  0.000000  0.142857         0.0      0.0  0.142857  0.000000   \n",
            "2     0.00  0.000000  0.000000         0.0      0.0  0.000000  0.166667   \n",
            "3     0.00  0.000000  0.000000         0.2      0.2  0.000000  0.000000   \n",
            "4     0.00  0.142857  0.000000         0.0      0.0  0.142857  0.000000   \n",
            "\n",
            "   learning   machine  models   natural       nlp        of      part  \\\n",
            "0  0.250000  0.250000     0.0  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.285714  0.142857     0.0  0.000000  0.000000  0.142857  0.142857   \n",
            "2  0.166667  0.166667     0.0  0.166667  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000     0.2  0.000000  0.200000  0.000000  0.000000   \n",
            "4  0.000000  0.000000     0.0  0.000000  0.142857  0.142857  0.142857   \n",
            "\n",
            "   processing      text      uses  word  \n",
            "0    0.000000  0.000000  0.000000   0.0  \n",
            "1    0.000000  0.000000  0.000000   0.0  \n",
            "2    0.166667  0.000000  0.166667   0.0  \n",
            "3    0.000000  0.000000  0.000000   0.2  \n",
            "4    0.142857  0.142857  0.000000   0.0  \n",
            "\n",
            "TF-IDF:\n",
            "     amazing   crucial      deep  embeddings   improve        is  language  \\\n",
            "0  0.652948  0.000000  0.000000    0.000000  0.000000  0.437287  0.000000   \n",
            "1  0.000000  0.000000  0.447531    0.000000  0.000000  0.299717  0.000000   \n",
            "2  0.000000  0.000000  0.000000    0.000000  0.000000  0.000000  0.468913   \n",
            "3  0.000000  0.000000  0.000000    0.463693  0.463693  0.000000  0.000000   \n",
            "4  0.000000  0.444898  0.000000    0.000000  0.000000  0.297954  0.000000   \n",
            "\n",
            "   learning   machine    models   natural       nlp        of      part  \\\n",
            "0  0.437287  0.437287  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.599433  0.299717  0.000000  0.000000  0.000000  0.361065  0.361065   \n",
            "2  0.314037  0.314037  0.000000  0.468913  0.000000  0.000000  0.000000   \n",
            "3  0.000000  0.000000  0.463693  0.000000  0.374105  0.000000  0.000000   \n",
            "4  0.000000  0.000000  0.000000  0.000000  0.358941  0.358941  0.358941   \n",
            "\n",
            "   processing      text      uses      word  \n",
            "0    0.000000  0.000000  0.000000  0.000000  \n",
            "1    0.000000  0.000000  0.000000  0.000000  \n",
            "2    0.378316  0.000000  0.468913  0.000000  \n",
            "3    0.000000  0.000000  0.000000  0.463693  \n",
            "4    0.358941  0.444898  0.000000  0.000000  \n",
            "\n",
            "Word2Vec Vector for 'machine':\n",
            " [ 0.07380386 -0.01533254 -0.04535943  0.06554627 -0.04860682 -0.01816819\n",
            "  0.02876629  0.00992307 -0.08285654 -0.09449372]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download tokenizer (if needed)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample dataset\n",
        "documents = [\n",
        "    \"Machine learning is amazing\",\n",
        "    \"Deep learning is a part of machine learning\",\n",
        "    \"Natural language processing uses machine learning\",\n",
        "    \"Word embeddings improve NLP models\",\n",
        "    \"Text processing is a crucial part of NLP\"\n",
        "]\n",
        "\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(documents)\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag-of-Words (BoW):\\n\", bow_df)\n",
        "\n",
        "\n",
        "tf_matrix = bow_matrix.toarray() / bow_matrix.sum(axis=1).reshape(-1, 1)\n",
        "tf_df = pd.DataFrame(tf_matrix, columns=vectorizer.get_feature_names_out())\n",
        "print(\"\\nNormalized Term Frequency (TF):\\n\", tf_df)\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\nTF-IDF:\\n\", tfidf_df)\n",
        "\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
        "word2vec_model = Word2Vec(sentences=tokenized_docs, vector_size=10, window=3, min_count=1, workers=4)\n",
        "word_vectors = {word: word2vec_model.wv[word] for word in word2vec_model.wv.index_to_key}\n",
        "\n",
        "print(\"\\nWord2Vec Vector for 'machine':\\n\", word_vectors.get('machine', \"Not in vocabulary\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptOpd08UZY55"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}